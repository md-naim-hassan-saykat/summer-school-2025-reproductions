{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a10002",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db1b40",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple 5x5 gridworld\n",
    "# S at (0,0), G at (4,4), reward +1 at goal, -0.01 per step\n",
    "# Actions: 0=up,1=right,2=down,3=left\n",
    "N = 5\n",
    "S = (0,0)\n",
    "G = (4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5466a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    r, c = state\n",
    "    if action == 0: r = max(0, r-1)\n",
    "    elif action == 1: c = min(N-1, c+1)\n",
    "    elif action == 2: r = min(N-1, r+1)\n",
    "    elif action == 3: c = max(0, c-1)\n",
    "    ns = (r,c)\n",
    "    if ns == G:\n",
    "        return ns, 1.0, True\n",
    "    return ns, -0.01, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb6e7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def eps_greedy(Q, s, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return np.random.randint(4)\n",
    "    r, c = s\n",
    "    return np.argmax(Q[r,c,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 800\n",
    "alpha = 0.5\n",
    "gamma = 0.98\n",
    "eps_start, eps_end = 0.9, 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((N,N,4))\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe89938",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(episodes):\n",
    "    eps = eps_end + (eps_start - eps_end)*np.exp(-ep/200)\n",
    "    s = S\n",
    "    done = False\n",
    "    ep_rew = 0.0\n",
    "    steps = 0\n",
    "    while not done and steps < 200:\n",
    "        a = eps_greedy(Q, s, eps)\n",
    "        ns, r, done = step(s, a)\n",
    "        r0,c0 = s\n",
    "        r1,c1 = ns\n",
    "        best_next = np.max(Q[r1,c1,:])\n",
    "        td_target = r + gamma*best_next*(0 if done else 1)\n",
    "        Q[r0,c0,a] += alpha*(td_target - Q[r0,c0,a])\n",
    "        s = ns\n",
    "        ep_rew += r\n",
    "        steps += 1\n",
    "    rewards.append(ep_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running average reward\n",
    "window = 20\n",
    "ra = np.convolve(rewards, np.ones(window)/window, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards, linewidth=1, label='Episode reward')\n",
    "plt.plot(np.arange(window-1, episodes), ra, linewidth=2, label='Running avg')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Q-learning in 5x5 Gridworld (no external frameworks)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('gridworld_qlearning_rewards.png', dpi=150)\n",
    "print('Saved figure: gridworld_qlearning_rewards.png')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
