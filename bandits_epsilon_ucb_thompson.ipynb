{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f65a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc0eeb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ----- Problem setup -----\n",
    "K = 5  # number of arms\n",
    "T = 2000  # horizon\n",
    "true_means = np.random.rand(K)  # Bernoulli means in [0,1]\n",
    "best_mean = np.max(true_means)\n",
    "best_arm = np.argmax(true_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbddb53",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ----- Helper: simulate pulls -----\n",
    "def bernoulli(p, size=1):\n",
    "    return (np.random.rand(size) < p).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10abe20",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ----- Îµ-greedy -----\n",
    "def run_epsilon_greedy(eps=0.1):\n",
    "    counts = np.zeros(K)\n",
    "    rewards = np.zeros(K)\n",
    "    cum_regret = np.zeros(T)\n",
    "    total_regret = 0.0\n",
    "    for t in range(T):\n",
    "        if np.random.rand() < eps:\n",
    "            a = np.random.randint(K)\n",
    "        else:\n",
    "            avg = np.divide(rewards, np.maximum(counts, 1), where=(np.maximum(counts,1)!=0))\n",
    "            a = np.argmax(avg)\n",
    "        r = bernoulli(true_means[a])[0]\n",
    "        counts[a] += 1\n",
    "        rewards[a] += r\n",
    "        total_regret += (best_mean - true_means[a])\n",
    "        cum_regret[t] = total_regret\n",
    "    return cum_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5fe24f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ----- UCB1 -----\n",
    "def run_ucb1():\n",
    "    counts = np.zeros(K)\n",
    "    rewards = np.zeros(K)\n",
    "    avg = np.zeros(K)\n",
    "    cum_regret = np.zeros(T)\n",
    "    total_regret = 0.0\n",
    "\n",
    "    # initialize: pull each arm once\n",
    "    for a in range(K):\n",
    "        r = bernoulli(true_means[a])[0]\n",
    "        counts[a] += 1\n",
    "        rewards[a] += r\n",
    "        avg[a] = rewards[a] / counts[a]\n",
    "        total_regret += (best_mean - true_means[a])\n",
    "        cum_regret[int(a)] = total_regret\n",
    "\n",
    "    for t in range(K, T):\n",
    "        ucb = avg + np.sqrt(2*np.log(t+1) / counts)\n",
    "        a = np.argmax(ucb)\n",
    "        r = bernoulli(true_means[a])[0]\n",
    "        counts[a] += 1\n",
    "        rewards[a] += r\n",
    "        avg[a] = rewards[a] / counts[a]\n",
    "        total_regret += (best_mean - true_means[a])\n",
    "        cum_regret[t] = total_regret\n",
    "    return cum_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52cc3f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ----- Thompson Sampling (Beta-Bernoulli) -----\n",
    "def run_thompson():\n",
    "    alpha = np.ones(K)  # prior Beta(1,1)\n",
    "    beta = np.ones(K)\n",
    "    cum_regret = np.zeros(T)\n",
    "    total_regret = 0.0\n",
    "    for t in range(T):\n",
    "        samples = np.random.beta(alpha, beta)\n",
    "        a = np.argmax(samples)\n",
    "        r = bernoulli(true_means[a])[0]\n",
    "        alpha[a] += r\n",
    "        beta[a] += (1 - r)\n",
    "        total_regret += (best_mean - true_means[a])\n",
    "        cum_regret[t] = total_regret\n",
    "    return cum_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7292c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = run_epsilon_greedy(0.1)\n",
    "ucb = run_ucb1()\n",
    "ts = run_thompson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(eg, label=r'$\\epsilon$-greedy (0.1)')\n",
    "plt.plot(ucb, label='UCB1')\n",
    "plt.plot(ts, label='Thompson Sampling')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Cumulative regret')\n",
    "plt.title('K-armed Bernoulli Bandit (K=%d)' % K)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('bandit_cumulative_regret.png', dpi=150)\n",
    "print('Saved figure: bandit_cumulative_regret.png')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
